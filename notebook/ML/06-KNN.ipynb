{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (kNN)\n",
    "\n",
    "IT is a type of instance-based learning or lazy learning where the function is only approximated locally and all computation is deferred until classification. Here's a detailed explanation:\n",
    "\n",
    " Core Concepts:\n",
    "\n",
    "1. **Distance Metric**:\n",
    "   - kNN relies on a distance metric to calculate how close points are. Common metrics include:\n",
    "     - **Euclidean Distance**: Most common for continuous variables.\n",
    "     - **Manhattan Distance**: Useful in scenarios where moving in one direction involves different costs than another, like city block distance.\n",
    "     - **Hamming Distance**: For categorical variables, counts the number of positions at which the corresponding symbols are different.\n",
    "\n",
    "2. **k Selection**:\n",
    "   - 'k' is the number of neighbors that contribute to the classification of a new instance. Choosing k is critical:\n",
    "     - Small k can make the model sensitive to noise (overfitting).\n",
    "     - Large k can smooth out local variations (underfitting).\n",
    "\n",
    "3. **Voting (for classification)**:\n",
    "   - For classification, the class of the new instance is decided by the majority vote among its k nearest neighbors.\n",
    "\n",
    "4. **Averaging (for regression)**:\n",
    "   - For regression tasks, the prediction is often the average of the target values of the k nearest neighbors.\n",
    "\n",
    " Algorithm Steps:\n",
    "\n",
    "1. **Store Training Data**:\n",
    "   - Unlike other algorithms that build a model during training, kNN simply stores all data points.\n",
    "\n",
    "2. **Calculate Distance**:\n",
    "   - When a new instance comes for prediction, compute the distance from this instance to all stored instances.\n",
    "\n",
    "3. **Find k-Nearest Neighbors**:\n",
    "   - Sort these distances and select the k smallest distances.\n",
    "\n",
    "4. **Predict**:\n",
    "   - For classification, take a vote among these neighbors. For regression, compute the average of their target values.\n",
    "\n",
    " Example:\n",
    "\n",
    "Let's consider a simple scenario for classifying whether a fruit is an apple or an orange based on two features: weight and texture.\n",
    "\n",
    "**Training Data:**\n",
    "\n",
    "| Fruit  | Weight (g) | Texture (1-5 scale, 1 being smooth) |\n",
    "|--------|------------|-------------------------------------|\n",
    "| Apple  | 150        | 4                                   |\n",
    "| Apple  | 170        | 3                                   |\n",
    "| Orange | 130        | 2                                   |\n",
    "| Orange | 160        | 3                                   |\n",
    "| Apple  | 140        | 4                                   |\n",
    "\n",
    "**New Instance to Classify:**\n",
    "\n",
    "- Weight: 155g\n",
    "- Texture: 3\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Compute Distances** (using Euclidean for simplicity):\n",
    "   - Distance to Apple(150, 4): $\\sqrt{(155-150)^2 + (3-4)^2} \\approx 5.1$\n",
    "   - Distance to Apple(170, 3): $\\sqrt{(155-170)^2 + (3-3)^2} = 15$\n",
    "   - Distance to Orange(130, 2): $\\sqrt{(155-130)^2 + (3-2)^2} \\approx 25.8$\n",
    "   - Distance to Orange(160, 3): $\\sqrt{(155-160)^2 + (3-3)^2} = 5$\n",
    "   - Distance to Apple(140, 4): $\\sqrt{(155-140)^2 + (3-4)^2} \\approx 15.8$\n",
    "\n",
    "2. **Select k=3 Nearest Neighbors**:\n",
    "   - Apple (150, 4) - 5.1\n",
    "   - Orange (160, 3) - 5\n",
    "   - Apple (170, 3) - 15\n",
    "   Since we're using k=3, we'll take these three points.\n",
    "\n",
    "3. **Majority Vote**:\n",
    "   - 2 Apples, 1 Orange. Therefore, the new fruit is classified as an **Apple**.\n",
    "\n",
    " Practical Considerations:\n",
    "\n",
    "- **Choice of k**:\n",
    "  - Often chosen via cross-validation to balance between underfitting and overfitting.\n",
    "\n",
    "- **Distance Weighting**:\n",
    "  - Points closer to the query point can be given more influence in the vote or average.\n",
    "\n",
    "- **Feature Scaling**:\n",
    "  - Important because kNN is sensitive to scale; features should be normalized or standardized.\n",
    "\n",
    "- **Memory and Time Complexity**:\n",
    "  - kNN needs to store all training data, which can be memory-intensive. Prediction time increases with data size since distances to all points need calculation.\n",
    "\n",
    "- **Curse of Dimensionality**:\n",
    "  - As the number of dimensions increases, the volume of the space increases so fast that the available data becomes sparse. This can make distance measures less\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions\n",
    "\n",
    "1. **What is kNN?**\n",
    "\n",
    "   kNN, or k-Nearest Neighbors, is a non-parametric, lazy learning algorithm used for classification and regression. It classifies a new instance based on the majority vote of its k nearest neighbors in the feature space for classification or averages their values for regression.\n",
    "\n",
    "2. **Why is kNN called a \"lazy learner\"?**\n",
    "\n",
    "   kNN is termed a \"lazy learner\" because it does not learn a discriminative function from the training data beforehand; instead, it memorizes the entire dataset and uses it directly for classification at prediction time. There's no explicit training phase other than storing the data.\n",
    "\n",
    "3. **How do you choose the value of k in kNN?**\n",
    "\n",
    "   The choice of k is typically made using cross-validation. A small k can lead to overfitting, while a large k might smooth out class boundaries too much, leading to underfitting. One rule of thumb is to choose k as the square root of the number of samples in the dataset, but empirical testing is key.\n",
    "\n",
    "4. **What distance metrics can be used in kNN?**\n",
    "\n",
    "   Common distance metrics include:\n",
    "   - **Euclidean Distance** for continuous data.\n",
    "   - **Manhattan Distance** for scenarios where moving along axes has different costs.\n",
    "   - **Hamming Distance** for categorical data.\n",
    "   - **Minkowski Distance** as a generalization of Euclidean and Manhattan.\n",
    "\n",
    "5. **How does kNN handle imbalanced datasets?**\n",
    "\n",
    "   kNN can struggle with imbalanced datasets since a majority vote might not represent the minority class well. Techniques like adjusting the number of neighbors (k), using weighted voting where votes from minority classes have more weight, or using different distance metrics can help manage this issue.\n",
    "\n",
    "6. **What are the advantages of kNN?**\n",
    "\n",
    "  - Simple to implement and understand.\n",
    "  - No training phase, which means new data can be added seamlessly.\n",
    "  - Works well with small datasets.\n",
    "  \n",
    "7. **What are the disadvantages of kNN?**\n",
    "\n",
    "  - Computationally expensive for large datasets as it requires distance calculations for all points.\n",
    "  - Sensitive to the scale of data; features need to be normalized.\n",
    "  - Prone to the curse of dimensionality where performance degrades as the number of features increases.\n",
    "  \n",
    "8. **How does scaling affect kNN performance?**\n",
    "   \n",
    "   Scaling is crucial for kNN because the algorithm uses distance metrics. Features on larger scales can dominate the distance calculation, leading to biased results. Normalization or standardization helps ensure all features contribute equally to the distance metric.\n",
    "\n",
    "9. **Can kNN be used for regression?**\n",
    "    \n",
    "    Yes, for regression, kNN predicts the value of a new instance by averaging the values of its k nearest neighbors. Sometimes, weights based on distance can be applied to give closer neighbors more influence.\n",
    " \n",
    "10. **How do you handle categorical variables in kNN?**\n",
    "    \n",
    "    For categorical variables, you can use distance metrics like Hamming distance for binary attributes or convert categorical variables into numerical ones using techniques like one-hot encoding, then apply traditional distance measures.\n",
    " \n",
    "11. **What's the impact of choosing too large or too small a k?**\n",
    "  - **Too small k:** The model might overfit, capturing noise as patterns.\n",
    "  - **Too large k:** The model might underfit, missing out on local patterns due to over-smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
