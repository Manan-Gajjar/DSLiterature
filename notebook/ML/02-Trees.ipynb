{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Decision Tree](#dt)\n",
    "2. [Random Foreset](#rf) \n",
    "3. [Implimentation from scratch](#impli)\n",
    "4. [Difference between Regression and classification](#diff)\n",
    "5. [OvA One Vs All](#ova)\n",
    "6. [Basic Interview Questions](#int)\n",
    "7. [Advance Interview Questions](#aint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dt'></a>\n",
    "# Decision Tree\n",
    "\n",
    "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It mimics human decision-making by splitting the data into branches based on certain conditions, starting from the root and ending at the leaves.\n",
    "\n",
    "### Key Components of a Decision Tree:\n",
    "\n",
    "1. **Root Node**: Represents the entire dataset and the starting point of the tree. It contains the most significant feature (split criterion).\n",
    "2. **Decision Nodes**: Intermediate nodes where the dataset is further split based on certain conditions.\n",
    "3. **Leaf Nodes**: The endpoints of the tree, representing the output or decision (e.g., a class label or predicted value).\n",
    "4. **Splitting**: The process of dividing a node into two or more sub-nodes based on a feature value or condition.\n",
    "5. **Pruning**: Reducing the size of the tree by removing parts that contribute little to predictive accuracy (used to prevent overfitting).\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. Start at the root node and evaluate the best feature to split the data using metrics such as:\n",
    "    - **Gini Impurity**: Measures the probability of incorrect classification at a node.\n",
    "    - **Entropy/Information Gain**: Measures the reduction in uncertainty after the split.\n",
    "    - **Variance Reduction**: For regression tasks, measures how well the split minimizes variability in the target variable.\n",
    "2. Repeat the splitting process for child nodes recursively until a stopping condition is met (e.g., no further improvement in splits or a maximum depth is reached).\n",
    "3. Use the leaves to make predictions.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- Easy to interpret and visualize.\n",
    "- Handles both numerical and categorical data.\n",
    "- Requires minimal data preprocessing.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "- Prone to overfitting with deep trees.\n",
    "- Sensitive to small changes in the dataset (leads to different splits and predictions).\n",
    "\n",
    "<a id='rf'></a>\n",
    "# Random Forest\n",
    "\n",
    "A Random Forest is an ensemble learning method that combines multiple decision trees to improve the accuracy, robustness, and generalization of the model.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Bootstrap Sampling**:\n",
    "    - From the original dataset, multiple random subsets (with replacement) are created.\n",
    "    - Each subset is used to train an individual decision tree.\n",
    "2. **Feature Subset Selection**:\n",
    "    - At each split in the decision trees, only a random subset of features is considered, which introduces diversity in the trees.\n",
    "3. **Voting/Averaging**:\n",
    "    - For classification: Each tree in the forest votes for a class, and the majority vote is selected as the final prediction.\n",
    "    - For regression: The average of all tree predictions is used as the final output.\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "1. **Number of Trees (n_estimators)**: The number of decision trees in the forest.\n",
    "2. **Maximum Features (max_features)**: The number of features considered for splitting at each node.\n",
    "3. **Maximum Depth (max_depth)**: Limits the depth of individual trees to avoid overfitting.\n",
    "4. **Min Samples Split (min_samples_split)**: Minimum number of samples required to split a node.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- Reduced Overfitting: By aggregating multiple trees, Random Forest mitigates the overfitting problem of individual decision trees.\n",
    "- Handles Missing Data: Can handle datasets with missing values effectively.\n",
    "- Feature Importance: Can rank features by importance, aiding interpretability.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "- Computationally expensive with a large number of trees.\n",
    "- Difficult to interpret compared to a single decision tree.\n",
    "\n",
    "### Comparison: Decision Tree vs. Random Forest\n",
    "\n",
    "| Aspect          | Decision Tree                  | Random Forest                              |\n",
    "|-----------------|--------------------------------|--------------------------------------------|\n",
    "| Structure       | Single tree                    | Multiple trees (ensemble)                  |\n",
    "| Overfitting     | Prone to overfitting           | Reduces overfitting by averaging predictions|\n",
    "| Accuracy        | May have lower accuracy        | Higher accuracy due to ensemble effect     |\n",
    "| Interpretability| Easy to interpret              | Harder to interpret due to multiple trees  |\n",
    "| Robustness      | Sensitive to data changes      | Robust to outliers and noise               |\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Decision Tree**: Useful when interpretability is critical or for quick prototyping.\n",
    "- **Random Forest**: Ideal for large datasets with complex patterns where accuracy is more important than interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='impli'></a>\n",
    "# Implementing Decision Tree and Random Forest from scratch\n",
    "### 1. Decision Tree Implementation\n",
    "\n",
    "**Steps:**\n",
    "1. Calculate the impurity metric (Gini Impurity or Entropy).\n",
    "2. Identify the best split (feature and threshold) based on impurity reduction.\n",
    "3. Recursively split the dataset until a stopping criterion is met (e.g., max depth or minimum samples).\n",
    "4. Use the resulting tree for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "   def __init__(self, max_depth=None, min_samples_split=2):\n",
    "       self.max_depth = max_depth\n",
    "       self.min_samples_split = min_samples_split\n",
    "       self.tree = None\n",
    "\n",
    "   def _gini(self, y):\n",
    "       classes, counts = np.unique(y, return_counts=True)\n",
    "       probs = counts / len(y)\n",
    "       return 1 - np.sum(probs ** 2)\n",
    "\n",
    "   def _split(self, X, y, feature_index, threshold):\n",
    "       left_indices = X[:, feature_index] <= threshold\n",
    "       right_indices = ~left_indices\n",
    "       return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "   def _best_split(self, X, y):\n",
    "       best_gain = 0\n",
    "       best_split = None\n",
    "\n",
    "       for feature_index in range(X.shape[1]):\n",
    "           thresholds = np.unique(X[:, feature_index])\n",
    "           for threshold in thresholds:\n",
    "               X_left, X_right, y_left, y_right = self._split(X, y, feature_index, threshold)\n",
    "               if len(y_left) == 0 or len(y_right) == 0:\n",
    "                   continue\n",
    "               gini_left = self._gini(y_left)\n",
    "               gini_right = self._gini(y_right)\n",
    "               weighted_gini = (len(y_left) * gini_left + len(y_right) * gini_right) / len(y)\n",
    "               gain = self._gini(y) - weighted_gini\n",
    "\n",
    "               if gain > best_gain:\n",
    "                   best_gain = gain\n",
    "                   best_split = {\"feature_index\": feature_index, \"threshold\": threshold,\n",
    "                                 \"X_left\": X_left, \"X_right\": X_right, \"y_left\": y_left, \"y_right\": y_right}\n",
    "\n",
    "       return best_split\n",
    "\n",
    "   def _build_tree(self, X, y, depth=0):\n",
    "       if len(y) < self.min_samples_split or (self.max_depth and depth >= self.max_depth) or len(np.unique(y)) == 1:\n",
    "           return np.bincount(y).argmax()\n",
    "\n",
    "       split = self._best_split(X, y)\n",
    "       if not split:\n",
    "           return np.bincount(y).argmax()\n",
    "\n",
    "       left = self._build_tree(split[\"X_left\"], split[\"y_left\"], depth + 1)\n",
    "       right = self._build_tree(split[\"X_right\"], split[\"y_right\"], depth + 1)\n",
    "\n",
    "       return {\"feature_index\": split[\"feature_index\"], \"threshold\": split[\"threshold\"], \"left\": left, \"right\": right}\n",
    "\n",
    "   def fit(self, X, y):\n",
    "       self.tree = self._build_tree(X, y)\n",
    "\n",
    "   def _predict(self, x, tree):\n",
    "       if not isinstance(tree, dict):\n",
    "           return tree\n",
    "       if x[tree[\"feature_index\"]] <= tree[\"threshold\"]:\n",
    "           return self._predict(x, tree[\"left\"])\n",
    "       else:\n",
    "           return self._predict(x, tree[\"right\"])\n",
    "\n",
    "   def predict(self, X):\n",
    "       return np.array([self._predict(x, self.tree) for x in X])\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[2.3, 1.2], [1.1, 3.4], [2.8, 3.5], [1.5, 0.7]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "print(tree.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Implementation\n",
    "\n",
    "**Steps:**\n",
    "1. Use bootstrapping to create multiple subsets of the training data.\n",
    "2. Train a decision tree on each subset.\n",
    "3. Use a random subset of features at each split in the trees.\n",
    "4. Aggregate predictions from all trees (majority vote for classification or average for regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForest:\n",
    "   def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, max_features=None):\n",
    "       self.n_estimators = n_estimators\n",
    "       self.max_depth = max_depth\n",
    "       self.min_samples_split = min_samples_split\n",
    "       self.max_features = max_features\n",
    "       self.trees = []\n",
    "\n",
    "   def _bootstrap_sample(self, X, y):\n",
    "       n_samples = X.shape[0]\n",
    "       indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "       return X[indices], y[indices]\n",
    "\n",
    "   def _random_features(self, X):\n",
    "       if not self.max_features:\n",
    "           return X, np.arange(X.shape[1])\n",
    "       feature_indices = np.random.choice(X.shape[1], self.max_features, replace=False)\n",
    "       return X[:, feature_indices], feature_indices\n",
    "\n",
    "   def fit(self, X, y):\n",
    "       self.trees = []\n",
    "       for _ in range(self.n_estimators):\n",
    "           X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "           X_subset, feature_indices = self._random_features(X_sample)\n",
    "           tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "           tree.fit(X_subset, y_sample)\n",
    "           self.trees.append((tree, feature_indices))\n",
    "\n",
    "   def predict(self, X):\n",
    "       tree_preds = []\n",
    "       for tree, feature_indices in self.trees:\n",
    "           X_subset = X[:, feature_indices]\n",
    "           tree_preds.append(tree.predict(X_subset))\n",
    "       tree_preds = np.array(tree_preds).T\n",
    "       return np.array([Counter(row).most_common(1)[0][0] for row in tree_preds])\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[2.3, 1.2], [1.1, 3.4], [2.8, 3.5], [1.5, 0.7]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "rf = RandomForest(n_estimators=5, max_depth=3, max_features=1)\n",
    "rf.fit(X, y)\n",
    "print(rf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"diff\"></a>\n",
    "**The primary difference in how a Decision Tree operates for classification vs. regression lies in the impurity measure** used for splitting and the way predictions are made. Here’s a detailed breakdown:\n",
    "\n",
    "1. **Splitting Criteria**\n",
    "\n",
    "    The process of choosing the best split is different for classification and regression tasks.\n",
    "\n",
    "    **Classification**\n",
    "\n",
    "    - **Objective:** Minimize the impurity of the target classes in each split.\n",
    "    - **Impurity Metrics:**\n",
    "      - **Gini Impurity:**\n",
    "         ${Gini = 1 - \\sum_{i=1}^{n} p_i^2}$\n",
    "         where ${p_i}$ is the proportion of samples of class ${i}$ in the node.\n",
    "      - **Entropy (Information Gain):**\n",
    "         ${\n",
    "         Entropy = - \\sum_{i=1}^{n} p_i \\log(p_i)\n",
    "         }$\n",
    "         The goal is to maximize the reduction in entropy (information gain) after the split.\n",
    "\n",
    "    **Regression**\n",
    "\n",
    "    - **Objective:** Minimize the variance or error of the target values in each split.\n",
    "    - **Metrics:**\n",
    "      - **Mean Squared Error (MSE):**\n",
    "         ${\n",
    "         MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "         }$\n",
    "         where ${ \\bar{y} }$ is the mean target value of the samples in the node.\n",
    "      - **Mean Absolute Error (MAE) (less common):**\n",
    "         ${\n",
    "         MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\bar{y}|\n",
    "         }$\n",
    "         The goal is to minimize the variance or error in target values after the split.\n",
    "\n",
    "2. **Leaf Node Output**\n",
    "\n",
    "    The way predictions are made in the leaf nodes differs between classification and regression.\n",
    "\n",
    "    **Classification**\n",
    "\n",
    "    - **Prediction:** The most common class (mode) in the leaf node.\n",
    "    - For example, if the leaf contains samples with classes ${[0, 0, 1]}$, the predicted class is ${0}$ (majority class).\n",
    "\n",
    "    **Regression**\n",
    "\n",
    "    - **Prediction:** The mean (average) of the target values in the leaf node.\n",
    "    - For example, if the leaf contains target values ${[2.5, 3.2, 2.7]}$, the prediction is the mean:\n",
    "      ${\n",
    "      \\bar{y} = \\frac{2.5 + 3.2 + 2.7}{3} = 2.8\n",
    "      }$\n",
    "\n",
    "3. **Evaluation of Split Quality**\n",
    "\n",
    "    The metrics used to evaluate splits differ between classification and regression.\n",
    "\n",
    "    **Classification**\n",
    "\n",
    "    - Splits are evaluated by the reduction in impurity:\n",
    "      - **Gini Impurity Decrease:**\n",
    "         ${\n",
    "         \\Delta Gini = Gini_{before} - Gini_{after}\n",
    "         }$\n",
    "      - **Information Gain (reduction in entropy):**\n",
    "         ${\n",
    "         \\Delta Entropy = Entropy_{before} - Entropy_{after}\n",
    "         }$\n",
    "\n",
    "    **Regression**\n",
    "\n",
    "    - Splits are evaluated by the reduction in variance or error:\n",
    "      - **Reduction in Variance (using MSE):**\n",
    "         ${\n",
    "         \\Delta Variance = Variance_{before} - Variance_{after}\n",
    "         }$\n",
    "\n",
    "4. **Handling Outputs**\n",
    "\n",
    "    The outputs for classification and regression tasks are handled differently due to the nature of the problem.\n",
    "\n",
    "    **Classification**\n",
    "\n",
    "    - **Discrete Labels:** The target values are categorical.\n",
    "    - **Probability Estimation:** Some implementations (e.g., Scikit-learn) can provide probabilities by calculating the proportion of each class in the leaf node.\n",
    "\n",
    "    **Regression**\n",
    "\n",
    "    - **Continuous Values:** The target values are real numbers.\n",
    "    - **Prediction Smoothing:** Some implementations may apply techniques like leaf regularization to prevent overfitting.\n",
    "\n",
    "5. **Example**\n",
    "\n",
    "    **Classification**\n",
    "\n",
    "    | Feature 1 | Feature 2 | Class |\n",
    "    |-----------|-----------|-------|\n",
    "    | 2.3       | 1.2       | 0     |\n",
    "    | 1.1       | 3.4       | 1     |\n",
    "    | 2.8       | 3.5       | 0     |\n",
    "    | 1.5       | 0.7       | 1     |\n",
    "\n",
    "    - The tree might split based on Gini or Entropy to separate class ${0}$ from class ${1}$.\n",
    "    - A leaf node predicts ${0}$ if it has more samples with class ${0}$.\n",
    "\n",
    "    **Regression**\n",
    "\n",
    "    | Feature 1 | Feature 2 | Target |\n",
    "    |-----------|-----------|--------|\n",
    "    | 2.3       | 1.2       | 2.5    |\n",
    "    | 1.1       | 3.4       | 3.2    |\n",
    "    | 2.8       | 3.5       | 2.7    |\n",
    "    | 1.5       | 0.7       | 3.8    |\n",
    "\n",
    "    - The tree splits to minimize the variance of target values within each leaf.\n",
    "    - A leaf node predicts the average of the target values.\n",
    "\n",
    "6. **Summary Table**\n",
    "\n",
    "    | Aspect                  | Classification                     | Regression                        |\n",
    "    |-------------------------|-------------------------------------|-----------------------------------|\n",
    "    | Target Type             | Discrete labels (e.g., 0, 1, 2)    | Continuous values (e.g., real numbers) |\n",
    "    | Impurity Metric         | Gini Impurity, Entropy             | Variance (MSE), MAE               |\n",
    "    | Prediction in Leaf Node | Most common class (mode)           | Mean (average) of target values   |\n",
    "    | Split Evaluation        | Reduction in Gini/Entropy          | Reduction in Variance/Error (MSE/MAE) |\n",
    "    | Output                  | Class label or probability         | Numeric value                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ova\"></a>\n",
    "## OvA (One-vs-All) is a strategy \n",
    "It used for solving multi-class classification problems by breaking them into multiple binary classification problems.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. For a dataset with ${ k }$ classes, the algorithm trains ${ k }$ separate binary classifiers.\n",
    "2. Each classifier is trained to distinguish one class (the “one”) from all the other classes (the “all”).\n",
    "3. During prediction:\n",
    "    - Each classifier produces a score (e.g., probability or decision boundary value).\n",
    "    - The class with the highest score across all classifiers is chosen as the final prediction.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose there are three classes: ${ A }$, ${ B }$, and ${ C }$. Using the OvA approach:\n",
    "\n",
    "- Classifier 1: Distinguishes ${ A }$ vs. ${ B }$ and ${ C }$.\n",
    "- Classifier 2: Distinguishes ${ B }$ vs. ${ A }$ and ${ C }$.\n",
    "- Classifier 3: Distinguishes ${ C }$ vs. ${ A }$ and ${ B }$.\n",
    "\n",
    "If given a new sample, the model evaluates all three classifiers and selects the class with the highest confidence.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- Simple and efficient for multi-class problems.\n",
    "- Works well with algorithms that are inherently binary, such as logistic regression or support vector machines.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "- Classifiers are independent, which can lead to inconsistencies in predictions.\n",
    "- May not perform as well as other strategies (e.g., One-vs-One) for highly imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"int\"></a>\n",
    "## Interview Questions\n",
    "\n",
    "Here’s a list of top interview questions on Decision Trees and Random Forests, along with short, concise answers:\n",
    "\n",
    "1. **What is a Decision Tree?**\n",
    "\n",
    "    A Decision Tree is a supervised learning algorithm that splits data into subsets based on feature values, forming a tree-like structure to make decisions for classification or regression tasks.\n",
    "\n",
    "2. **How does a Decision Tree decide where to split?**\n",
    "\n",
    "    It chooses the split that maximizes the reduction in impurity:\n",
    "    - For classification: Uses Gini Impurity or Entropy.\n",
    "    - For regression: Uses Variance Reduction (e.g., MSE).\n",
    "\n",
    "3. **What are the advantages of Decision Trees?**\n",
    "\n",
    "    - Easy to interpret and visualize.\n",
    "    - Handles both categorical and numerical data.\n",
    "    - Requires minimal data preprocessing (e.g., no scaling).\n",
    "\n",
    "4. **What are the limitations of Decision Trees?**\n",
    "\n",
    "    - Prone to overfitting, especially with deep trees.\n",
    "    - Sensitive to small changes in the data (unstable splits).\n",
    "    - Less accurate compared to ensemble methods like Random Forests.\n",
    "\n",
    "5. **What is Random Forest?**\n",
    "\n",
    "    Random Forest is an ensemble learning method that combines multiple decision trees (trained on random subsets of data and features) to improve accuracy and reduce overfitting.\n",
    "\n",
    "6. **Why is Random Forest better than a single Decision Tree?**\n",
    "\n",
    "    Random Forest:\n",
    "    - Reduces overfitting by averaging predictions from multiple trees.\n",
    "    - Improves generalization through randomness (bootstrap samples and random feature selection).\n",
    "\n",
    "7. **What is bootstrap aggregation (bagging) in Random Forest?**\n",
    "\n",
    "    Bagging involves:\n",
    "    1. Creating multiple bootstrap samples (random subsets with replacement) from the training data.\n",
    "    2. Training each tree on a different sample and combining their outputs (e.g., majority vote for classification, averaging for regression).\n",
    "\n",
    "8. **How does Random Forest handle feature selection?**\n",
    "\n",
    "    At each split, it randomly selects a subset of features to find the best split, reducing correlation between trees and improving diversity.\n",
    "\n",
    "9. **How does Random Forest handle missing data?**\n",
    "\n",
    "    Random Forest can:\n",
    "    - Use surrogate splits (alternative splits for missing data).\n",
    "    - Impute missing values based on proximity or feature importance.\n",
    "\n",
    "10. **What are Out-of-Bag (OOB) samples in Random Forest?**\n",
    "\n",
    "     OOB samples are data points not included in the bootstrap sample for a particular tree. These samples are used to estimate model accuracy without needing a separate validation set.\n",
    "\n",
    "11. **What is Gini Impurity?**\n",
    "\n",
    "     A measure of node impurity used for classification:\n",
    "     ${Gini = 1 - \\sum_{i=1}^{n} p_i^2}$\n",
    "     where ${p_i}$ is the proportion of samples belonging to class ${i}$.\n",
    "\n",
    "12. **What is the difference between Gini and Entropy?**\n",
    "\n",
    "     - Gini Impurity is faster to compute and ranges from 0 (pure) to 0.5 (max impurity for binary classes).\n",
    "     - Entropy measures information gain and ranges from 0 (pure) to ${\\log_2(c)}$ where ${ c }$ is the number of classes.\n",
    "\n",
    "13. **What are the hyperparameters of Decision Trees?**\n",
    "\n",
    "     Key hyperparameters include:\n",
    "     - `max_depth`: Maximum depth of the tree.\n",
    "     - `min_samples_split`: Minimum samples required to split a node.\n",
    "     - `min_samples_leaf`: Minimum samples required in a leaf node.\n",
    "     - `criterion`: Splitting metric (e.g., Gini, Entropy).\n",
    "\n",
    "14. **What are the hyperparameters of Random Forest?**\n",
    "\n",
    "     Key hyperparameters include:\n",
    "     - `n_estimators`: Number of trees in the forest.\n",
    "     - `max_features`: Number of features to consider for splits.\n",
    "     - `max_depth`: Maximum depth of each tree.\n",
    "     - `bootstrap`: Whether to use bootstrap sampling.\n",
    "\n",
    "15. **How does Random Forest prevent overfitting?**\n",
    "\n",
    "     - Averages predictions from multiple trees.\n",
    "     - Adds randomness via bootstrap samples and random feature selection.\n",
    "\n",
    "16. **What is the difference between Random Forest and Bagging?**\n",
    "\n",
    "     - Bagging trains trees on different bootstrap samples but uses all features for splits.\n",
    "     - Random Forest adds randomness by selecting a subset of features at each split.\n",
    "\n",
    "17. **Can Decision Trees handle multi-class classification?**\n",
    "\n",
    "     Yes, Decision Trees handle multi-class classification by evaluating impurity metrics across all classes.\n",
    "\n",
    "18. **What are feature importances in Random Forest?**\n",
    "\n",
    "     Feature importance measures how much each feature contributes to reducing impurity in the forest. Features with higher importance have more influence on the predictions.\n",
    "\n",
    "19. **How does Random Forest handle overfitting?**\n",
    "\n",
    "     By averaging the outputs of multiple trees and introducing randomness in data and feature selection, Random Forest reduces variance and minimizes overfitting.\n",
    "\n",
    "20. **What are the limitations of Random Forest?**\n",
    "\n",
    "     - Computationally expensive (requires many trees).\n",
    "     - Difficult to interpret compared to a single Decision Tree.\n",
    "     - May not perform well on high-dimensional sparse data.\n",
    "\n",
    "21. **What is pruning in Decision Trees?**\n",
    "\n",
    "     Pruning involves cutting back the tree to reduce overfitting:\n",
    "     - Pre-pruning: Stop tree growth early (e.g., limit `max_depth`).\n",
    "     - Post-pruning: Remove branches after full tree growth.\n",
    "\n",
    "22. **How is the prediction made in Random Forest?**\n",
    "\n",
    "     - Classification: Majority vote across all trees.\n",
    "     - Regression: Average of predictions from all trees.\n",
    "\n",
    "23. **How does Random Forest deal with imbalanced datasets?**\n",
    "\n",
    "     - Assign class weights during training.\n",
    "     - Use subsampling or SMOTE to balance classes.\n",
    "\n",
    "24. **What are OOB error estimates?**\n",
    "\n",
    "     Out-of-Bag error is an unbiased estimate of the model’s accuracy, calculated using predictions for OOB samples across all trees.\n",
    "\n",
    "25. **Can Random Forest handle high-dimensional data?**\n",
    "\n",
    "     Yes, Random Forest can handle high-dimensional data efficiently due to random feature selection, but feature selection or dimensionality reduction may improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aint\"></a>\n",
    "### Advance Interview Questions.\n",
    "\n",
    "1. **What is the difference between variance reduction and impurity reduction in Decision Trees?**\n",
    "\n",
    "    - **Variance reduction (for regression):** Measures how much the variability in the target is reduced after a split. It uses metrics like Mean Squared Error (MSE).\n",
    "    - **Impurity reduction (for classification):** Measures how “pure” a split is in terms of class distribution. Metrics include Gini Impurity or Entropy.\n",
    "\n",
    "2. **Why is Random Forest not prone to overfitting, unlike Decision Trees?**\n",
    "\n",
    "    - Random Forest reduces overfitting by averaging predictions of multiple trees, which lowers variance.\n",
    "    - It uses randomness in two ways:\n",
    "      - Bootstrap sampling (different subsets of data for each tree).\n",
    "      - Random feature selection at each split.\n",
    "\n",
    "3. **What is the time complexity of training and predicting with a Decision Tree?**\n",
    "\n",
    "    - **Training:** \\(O(n \\log n)\\), where \\(n\\) is the number of samples and \\(d\\) is the number of features.\n",
    "    - **Prediction:** \\(O(\\log n)\\), as it traverses from the root to a leaf.\n",
    "\n",
    "4. **What are some common problems in building Decision Trees, and how can you address them?**\n",
    "\n",
    "    1. **Overfitting:**\n",
    "        - Use pruning (max_depth, min_samples_split).\n",
    "        - Switch to ensemble methods like Random Forest.\n",
    "    2. **Bias from irrelevant features:**\n",
    "        - Remove irrelevant/noisy features via feature selection.\n",
    "    3. **Imbalanced data:**\n",
    "        - Use class weights or oversample the minority class.\n",
    "\n",
    "5. **How does Random Forest handle correlated features?**\n",
    "\n",
    "    - Random Forest struggles with highly correlated features because similar splits can appear in multiple trees, reducing diversity.\n",
    "    - To address this, reduce correlations using PCA, or rely on feature importance scores to eliminate redundant features.\n",
    "\n",
    "6. **Why is Random Forest not ideal for extrapolation?**\n",
    "\n",
    "    Random Forest only makes predictions within the range of the training data, as trees learn by splitting the existing feature space. It cannot predict outside this range, making it unsuitable for extrapolation tasks.\n",
    "\n",
    "7. **How is feature importance calculated in Random Forest?**\n",
    "\n",
    "    - **Gini-based importance:** Measures the reduction in Gini Impurity caused by each feature across all splits in the forest.\n",
    "    - **Permutation importance:** Evaluates how randomizing a feature affects model accuracy (proxy for feature relevance).\n",
    "\n",
    "8. **What is the role of randomness in Random Forest?**\n",
    "\n",
    "    - Random Forest introduces randomness in:\n",
    "      - Data sampling: Each tree is trained on a bootstrap sample.\n",
    "      - Feature selection: A random subset of features is considered for each split.\n",
    "    - This randomness ensures low correlation between trees and improves generalization.\n",
    "\n",
    "9. **What is the tradeoff between max_features and model performance in Random Forest?**\n",
    "\n",
    "    - **Low max_features:**\n",
    "      - Increases tree diversity (reduces correlation).\n",
    "      - May lead to underfitting if important features are missed.\n",
    "    - **High max_features:**\n",
    "      - Reduces diversity (increases correlation between trees).\n",
    "      - May overfit if trees become too similar.\n",
    "\n",
    "10. **How does Random Forest handle high-dimensional data compared to SVM?**\n",
    "\n",
    "     - **Random Forest:** Efficient for high-dimensional data because it selects a subset of features at each split. However, it may struggle with sparsity.\n",
    "     - **SVM:** Handles high-dimensional sparse data better, especially with the right kernel (e.g., RBF).\n",
    "\n",
    "11. **Explain the difference between “max_depth” and “min_samples_split” in Decision Trees.**\n",
    "\n",
    "     - **max_depth:** Limits the depth of the tree, controlling overfitting by stopping growth early.\n",
    "     - **min_samples_split:** The minimum number of samples required to split a node. Higher values prevent splits on small subsets, which also reduces overfitting.\n",
    "\n",
    "12. **How does Random Forest handle outliers?**\n",
    "\n",
    "     - Decision Trees in Random Forest tend to ignore outliers because splits are determined based on the majority of samples, not extreme values. However:\n",
    "        - Outliers can still affect the bootstrap samples if included multiple times.\n",
    "\n",
    "13. **How does pruning work in Decision Trees?**\n",
    "\n",
    "     - **Pre-pruning:** Stops tree growth early (e.g., limiting max_depth or requiring min_samples_split).\n",
    "     - **Post-pruning:** Fully grows the tree, then removes branches with low significance (e.g., based on validation error).\n",
    "\n",
    "14. **What are surrogate splits, and why are they useful?**\n",
    "\n",
    "     Surrogate splits are backup splits used when data for the primary splitting feature is missing. They improve robustness in handling missing values by using other correlated features for the same split.\n",
    "\n",
    "15. **Why is Random Forest slower than a single Decision Tree?**\n",
    "\n",
    "     - Random Forest trains multiple trees (typically \\(n\\)), requiring more computation.\n",
    "     - Prediction involves combining results from all trees, increasing latency.\n",
    "\n",
    "16. **How would you explain OOB error to a non-technical person?**\n",
    "\n",
    "     OOB (Out-of-Bag) error is a way to measure how well the model performs without needing a separate validation dataset. It uses data not seen by each tree during training to evaluate accuracy.\n",
    "\n",
    "17. **What happens if all features are perfectly correlated in Random Forest?**\n",
    "\n",
    "     - Trees become similar, as they will repeatedly split on the same features, reducing diversity.\n",
    "     - Random Forest may lose its advantage over a single Decision Tree.\n",
    "\n",
    "18. **How can you tune hyperparameters in Random Forest for better performance?**\n",
    "\n",
    "     - Use grid search or random search to optimize key parameters:\n",
    "        - n_estimators: Number of trees.\n",
    "        - max_features: Features to consider at each split.\n",
    "        - max_depth: Depth of trees.\n",
    "        - min_samples_split and min_samples_leaf: Minimum samples for splits and leaf nodes.\n",
    "\n",
    "19. **What is the difference between Extra Trees (Extremely Randomized Trees) and Random Forest?**\n",
    "\n",
    "     - **Random Forest:** Selects the best split among a subset of features.\n",
    "     - **Extra Trees:** Splits randomly within a subset of features, increasing speed but reducing precision.\n",
    "\n",
    "20. **How would you evaluate feature importance using Random Forest in Python?**\n",
    "\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "     model = RandomForestClassifier()\n",
    "     model.fit(X, y)\n",
    "\n",
    "     print(model.feature_importances_)\n",
    "     ```\n",
    "\n",
    "21. **How does Random Forest handle imbalanced datasets?**\n",
    "\n",
    "     - Assign class weights (class_weight=\"balanced\") to penalize the majority class.\n",
    "     - Use techniques like SMOTE (Synthetic Minority Oversampling) to balance the dataset.\n",
    "\n",
    "22. **Explain the curse of dimensionality in the context of Decision Trees and Random Forests.**\n",
    "\n",
    "     High-dimensional data may:\n",
    "     - Lead to sparse splits, making it harder for trees to find meaningful patterns.\n",
    "     - Increase computation time in Random Forest due to more features being evaluated.\n",
    "\n",
    "23. **What are the differences between Gradient Boosting and Random Forest?**\n",
    "\n",
    "     | Aspect           | Random Forest                     | Gradient Boosting                        |\n",
    "     |------------------|-----------------------------------|------------------------------------------|\n",
    "     | Method           | Bagging (averages multiple trees) | Boosting (sequentially corrects errors)  |\n",
    "     | Speed            | Faster due to parallel training   | Slower, as trees are built sequentially  |\n",
    "     | Overfitting      | Less prone to overfitting         | Prone to overfitting without tuning      |\n",
    "\n",
    "24. **What is a limitation of Random Forest’s feature importance scores?**\n",
    "\n",
    "     They can be biased toward features with many unique values (e.g., continuous variables) compared to categorical features with fewer splits.\n",
    "\n",
    "25. **How would you explain Decision Trees and Random Forests to a layperson?**\n",
    "\n",
    "     - **Decision Tree:** Think of a flowchart where each question splits data into smaller groups until a final decision is made.\n",
    "     - **Random Forest:** Imagine asking multiple experts (decision trees) the same question and combining their answers for better accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
