{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes Classifier is a supervised machine learning algorithm based on Bayes’ Theorem. It is primarily used for classification tasks and assumes that the features are independent of each other (hence the term “naive”). Despite this simplifying assumption, Naive Bayes often performs surprisingly well for many real-world problems.\n",
    "\n",
    "### Bayes’ Theorem\n",
    "\n",
    "Bayes’ theorem provides a way to calculate the conditional probability of an event ${ A }$, given that event ${ B }$ has occurred. The formula is:\n",
    "\n",
    "${ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} }$\n",
    "\n",
    "Where:\n",
    "\n",
    "- ${ P(A|B) }$: Posterior probability (probability of ${ A }$ given ${ B }$).\n",
    "- ${ P(B|A) }$: Likelihood (probability of ${ B }$ given ${ A }$).\n",
    "- ${ P(A) }$: Prior probability of ${ A }$.\n",
    "- ${ P(B) }$: Marginal probability of ${ B }$.\n",
    "\n",
    "For Naive Bayes, this is extended to multiple features.\n",
    "\n",
    "### How Naive Bayes Works\n",
    "\n",
    "The algorithm classifies a data point ${ X = \\{x_1, x_2, \\ldots, x_n\\} }$ (with ${ n }$ features) into a class ${ C_k }$. According to Bayes’ theorem:\n",
    "\n",
    "${ P(C_k | X) = \\frac{P(X | C_k) \\cdot P(C_k)}{P(X)} }$\n",
    "\n",
    "1. **Prior ( ${ P(C_k) }$ ):** The probability of a class ${ C_k }$ before observing the data (based on class frequencies in the training data).\n",
    "2. **Likelihood ( ${ P(X|C_k) }$ ):** The probability of the data ${ X }$ given the class ${ C_k }$.\n",
    "3. **Marginal Probability ( ${ P(X) }$ ):** The total probability of observing ${ X }$ (irrelevant for classification since it is constant across classes).\n",
    "\n",
    "The algorithm predicts the class with the highest posterior probability ${ P(C_k|X) }$.\n",
    "\n",
    "### Naive Assumption\n",
    "\n",
    "The “naive” assumption is that all features are conditionally independent given the class label. This simplifies the likelihood ${ P(X|C_k) }$ into:\n",
    "\n",
    "${ P(X|C_k) = P(x_1|C_k) \\cdot P(x_2|C_k) \\cdot \\ldots \\cdot P(x_n|C_k) }$\n",
    "\n",
    "### Steps in Naive Bayes Classification\n",
    "\n",
    "1. **Calculate Priors:** Compute ${ P(C_k) }$, the proportion of instances in each class.\n",
    "2. **Compute Likelihoods:** Compute ${ P(x_i|C_k) }$ for each feature ${ x_i }$ given the class ${ C_k }$.\n",
    "3. **Compute Posterior:** For a given instance ${ X }$, compute ${ P(C_k|X) }$ for each class ${ C_k }$.\n",
    "4. **Classify:** Assign the instance to the class with the highest posterior probability.\n",
    "\n",
    "### Types of Naive Bayes Classifiers\n",
    "\n",
    "1. **Gaussian Naive Bayes:** Assumes that the data follows a Gaussian (normal) distribution.\n",
    "2. **Multinomial Naive Bayes:** Used for discrete data, such as word counts in text classification.\n",
    "3. **Bernoulli Naive Bayes:** Designed for binary/Boolean features.\n",
    "\n",
    "### Example: Spam Email Classification\n",
    "\n",
    "### Dataset\n",
    "\n",
    "| Email   | Word: “Free” | Word: “Money” | Word: “Offer” | Spam/Not Spam |\n",
    "|---------|--------------|---------------|---------------|---------------|\n",
    "| Email 1 | Yes          | Yes           | Yes           | Spam          |\n",
    "| Email 2 | No           | Yes           | No            | Not Spam      |\n",
    "| Email 3 | Yes          | No            | Yes           | Spam          |\n",
    "| Email 4 | No           | No            | No            | Not Spam      |\n",
    "\n",
    "### Step 1: Compute Priors\n",
    "\n",
    "${ P(Spam) = \\frac{2}{4} = 0.5, \\quad P(NotSpam) = \\frac{2}{4} = 0.5 }$\n",
    "\n",
    "### Step 2: Compute Likelihoods\n",
    "\n",
    "For ${ P(x_i|Spam) }$:\n",
    "\n",
    "- ${ P(\\text{“Free”}|Spam) = \\frac{2}{2} = 1 }$\n",
    "- ${ P(\\text{“Money”}|Spam) = \\frac{1}{2} = 0.5 }$\n",
    "- ${ P(\\text{“Offer”}|Spam) = \\frac{2}{2} = 1 }$\n",
    "\n",
    "For ${ P(x_i|NotSpam) }$:\n",
    "\n",
    "- ${ P(\\text{“Free”}|NotSpam) = \\frac{0}{2} = 0 }$\n",
    "- ${ P(\\text{“Money”}|NotSpam) = \\frac{1}{2} = 0.5 }$\n",
    "- ${ P(\\text{“Offer”}|NotSpam) = \\frac{0}{2} = 0 }$\n",
    "\n",
    "### Step 3: Classify New Email\n",
    "\n",
    "New Email: “Free Money Offer”\n",
    "\n",
    "Features: “Free = Yes”, “Money = Yes”, “Offer = Yes”\n",
    "\n",
    "Posterior for Spam:\n",
    "\n",
    "${ P(Spam | X) \\propto P(\\text{“Free”}|Spam) \\cdot P(\\text{“Money”}|Spam) \\cdot P(\\text{“Offer”}|Spam) \\cdot P(Spam) }$\n",
    "\n",
    "${ P(Spam | X) \\propto 1 \\cdot 0.5 \\cdot 1 \\cdot 0.5 = 0.25 }$\n",
    "\n",
    "Posterior for Not Spam:\n",
    "\n",
    "${ P(NotSpam | X) \\propto P(\\text{“Free”}|NotSpam) \\cdot P(\\text{“Money”}|NotSpam) \\cdot P(\\text{“Offer”}|NotSpam) \\cdot P(NotSpam) }$\n",
    "\n",
    "${ P(NotSpam | X) \\propto 0 \\cdot 0.5 \\cdot 0 \\cdot 0.5 = 0 }$\n",
    "\n",
    "**Prediction:** The email is classified as Spam since ${ P(Spam|X) > P(NotSpam|X) }$.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Fast and Efficient:** Works well with large datasets.\n",
    "2. **Handles Multi-class Problems:** Straightforward for multiple classes.\n",
    "3. **Scalable:** Requires only linear time for training and prediction.\n",
    "4. **Robust to Irrelevant Features:** Performs well even when some features are unrelated.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Naive Assumption of Independence:** Assumes all features are independent, which may not hold in practice.\n",
    "2. **Zero Probability Problem:** If a feature value never occurs in the training set, its likelihood becomes zero. (Solved by Laplace Smoothing.)\n",
    "3. **Not Suitable for Continuous Data:** Requires Gaussian assumption or discretization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions\n",
    "\n",
    "### 1. What is the Naive Bayes algorithm?\n",
    "\n",
    "Naive Bayes is a probabilistic classification algorithm based on Bayes’ Theorem. It assumes that features are conditionally independent given the class label. It is widely used for text classification, spam detection, and sentiment analysis.\n",
    "\n",
    "### 2. Why is Naive Bayes called “naive”?\n",
    "\n",
    "It is called “naive” because it assumes that all features are independent of each other, which is often unrealistic in real-world datasets. Despite this, it performs well in practice.\n",
    "\n",
    "### 3. What is Bayes’ Theorem?\n",
    "\n",
    "Bayes’ Theorem calculates the posterior probability as:\n",
    "\n",
    "${ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} }$\n",
    "\n",
    "Where ${ P(A) }$ is the prior probability, ${ P(B|A) }$ is the likelihood, and ${ P(B) }$ is the evidence.\n",
    "\n",
    "### 4. What are the types of Naive Bayes classifiers?\n",
    "\n",
    "1. **Gaussian Naive Bayes:** Assumes features follow a Gaussian distribution.\n",
    "2. **Multinomial Naive Bayes:** Used for discrete data like word counts (e.g., text classification).\n",
    "3. **Bernoulli Naive Bayes:** Handles binary/Boolean features.\n",
    "\n",
    "### 5. How does Naive Bayes handle continuous features?\n",
    "\n",
    "For continuous features, Naive Bayes uses the Gaussian distribution to compute likelihoods. The probability is calculated as:\n",
    "\n",
    "${ P(x|y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) }$\n",
    "\n",
    "where ${ \\mu }$ is the mean and ${ \\sigma^2 }$ is the variance of the feature for a given class.\n",
    "\n",
    "### 6. What is Laplace Smoothing, and why is it needed?\n",
    "\n",
    "Laplace Smoothing (also called Additive Smoothing) is used to handle the zero probability problem. If a feature value never appears in the training set for a class, its likelihood becomes zero. Laplace smoothing adds a small value ${ \\alpha }$ (usually 1) to all feature counts to avoid this issue.\n",
    "\n",
    "### 7. What are the key assumptions of Naive Bayes?\n",
    "\n",
    "1. All features are conditionally independent given the class label.\n",
    "2. Features contribute equally to the prediction.\n",
    "3. Data is distributed according to the assumptions of the specific Naive Bayes variant (e.g., Gaussian, multinomial).\n",
    "\n",
    "### 8. How does Naive Bayes handle multi-class classification?\n",
    "\n",
    "Naive Bayes directly handles multi-class classification by calculating the posterior probability ${ P(C_k|X) }$ for each class ${ C_k }$ and selecting the class with the highest probability.\n",
    "\n",
    "### 9. What are the advantages of Naive Bayes?\n",
    "\n",
    "- Simple and fast to implement.\n",
    "- Works well with high-dimensional data (e.g., text classification).\n",
    "- Effective with small datasets.\n",
    "- Robust to irrelevant features.\n",
    "\n",
    "### 10. What are the limitations of Naive Bayes?\n",
    "\n",
    "- Assumes feature independence, which is often violated.\n",
    "- Struggles with correlated features.\n",
    "- May perform poorly if feature distributions deviate from assumptions (e.g., non-Gaussian data).\n",
    "\n",
    "### 11. Explain the difference between Multinomial and Bernoulli Naive Bayes.\n",
    "\n",
    "- **Multinomial Naive Bayes:** Used for text data with word frequencies or counts.\n",
    "- **Bernoulli Naive Bayes:** Used for binary/Boolean features (e.g., whether a word exists or not in a document).\n",
    "\n",
    "### 12. How does Naive Bayes perform on imbalanced datasets?\n",
    "\n",
    "Naive Bayes can be biased towards the majority class in imbalanced datasets. Using class weights or oversampling/undersampling techniques can help mitigate this issue.\n",
    "\n",
    "### 13. Why is Naive Bayes popular for text classification?\n",
    "\n",
    "Naive Bayes is effective for text classification because:\n",
    "- It works well with sparse, high-dimensional data.\n",
    "- The independence assumption (e.g., word occurrences) often holds reasonably well in practice.\n",
    "- It’s computationally efficient for large text datasets.\n",
    "\n",
    "### 14. How is Naive Bayes different from Logistic Regression?\n",
    "\n",
    "- **Naive Bayes:** Based on Bayes’ Theorem and assumes feature independence.\n",
    "- **Logistic Regression:** Models the conditional probability using a sigmoid function and does not assume independence among features.\n",
    "\n",
    "### 15. Can Naive Bayes handle missing data?\n",
    "\n",
    "Naive Bayes can handle missing data by ignoring the feature with missing values during the probability calculation, as it treats features independently.\n",
    "\n",
    "### 16. How do you evaluate a Naive Bayes model?\n",
    "\n",
    "Use metrics such as:\n",
    "- Accuracy (for balanced datasets).\n",
    "- Precision, Recall, and F1-score (for imbalanced datasets).\n",
    "- Confusion Matrix: To understand misclassifications.\n",
    "- Cross-Validation: For robust performance evaluation.\n",
    "\n",
    "### 17. How does feature scaling affect Naive Bayes?\n",
    "\n",
    "Naive Bayes does not require feature scaling since it uses probabilities, not distance-based calculations.\n",
    "\n",
    "### 18. What are some real-world applications of Naive Bayes?\n",
    "\n",
    "- Spam Detection: Classifying emails as spam or not spam.\n",
    "- Sentiment Analysis: Determining the sentiment (positive/negative) of text.\n",
    "- Medical Diagnosis: Predicting diseases based on symptoms.\n",
    "- Document Categorization: Classifying news articles, books, or research papers.\n",
    "\n",
    "### 19. Why is Naive Bayes not suitable for correlated features?\n",
    "\n",
    "Naive Bayes assumes feature independence. When features are highly correlated, the independence assumption is violated, leading to biased probability estimates and poor performance.\n",
    "\n",
    "### 20. How can Naive Bayes handle continuous and categorical features in the same dataset?\n",
    "\n",
    "For continuous features, use Gaussian Naive Bayes.\n",
    "For categorical features, use Multinomial or Bernoulli Naive Bayes.\n",
    "If both types are present, preprocess the features (e.g., discretize continuous variables or use separate likelihood functions for each type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
