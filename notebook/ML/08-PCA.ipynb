{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving as much variance (information) as possible. It transforms the data into a new coordinate system defined by principal components (PCs), which are linear combinations of the original features.\n",
    "\n",
    "### How PCA Works:\n",
    "\n",
    "1. **Standardize the Data:** Ensure all features have a mean of 0 and variance of 1 to avoid bias from larger-scale features.\n",
    "2. **Compute Covariance Matrix:** Calculate the covariance matrix of the dataset to understand relationships between features.\n",
    "3. **Compute Eigenvectors and Eigenvalues:**\n",
    "    - Eigenvectors determine the directions (principal components) of maximum variance.\n",
    "    - Eigenvalues measure the amount of variance explained by each principal component.\n",
    "4. **Sort Principal Components:** Rank components by their eigenvalues in descending order.\n",
    "5. **Select Top Components:** Retain the first principal components that explain most of the variance.\n",
    "6. **Transform Data:** Project the original data onto the selected principal components.\n",
    "\n",
    "### PCA Formula:\n",
    "\n",
    "The projection of a data point $ x $ onto a principal component $ p $ is:\n",
    "\n",
    "$ x' = x \\cdot p $\n",
    "\n",
    "where $ p $ is the eigenvector of the covariance matrix.\n",
    "\n",
    "### Key Properties of PCA:\n",
    "\n",
    "- **Linear transformation:** PCA finds linear combinations of the original features.\n",
    "- **Variance maximization:** The first principal component explains the most variance, the second explains the next most (orthogonal to the first), and so on.\n",
    "- **Unsupervised:** PCA does not use labels.\n",
    "\n",
    "### Applications of PCA:\n",
    "\n",
    "- Reduce dimensionality for visualization (e.g., projecting data to 2D or 3D).\n",
    "- Speed up machine learning models by removing redundant features.\n",
    "- Denoise data by discarding components with low variance.\n",
    "- Feature extraction and compression.\n",
    "\n",
    "### Advantages of PCA:\n",
    "\n",
    "- Reduces overfitting by eliminating redundant features.\n",
    "- Enhances interpretability by reducing dimensions.\n",
    "- Speeds up computations for high-dimensional data.\n",
    "\n",
    "### Disadvantages of PCA:\n",
    "\n",
    "- Linear method, so it struggles with non-linear relationships.\n",
    "- Sensitive to scaling of data.\n",
    "- Can lose interpretability of original features.\n",
    "\n",
    "# t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "### What is t-SNE?\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique primarily used for visualizing high-dimensional data in 2D or 3D. It focuses on preserving the local structure of the data.\n",
    "\n",
    "### How t-SNE Works:\n",
    "\n",
    "1. **Pairwise Similarities in High-Dimensional Space:**\n",
    "\n",
    "    Compute the probability $ p_{ij} $ that a point $ i $ is a neighbor of point $ j $ in the original space using a Gaussian distribution:\n",
    "\n",
    "    $ p_{ij} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq l} \\exp(-\\|x_k - x_l\\|^2 / 2\\sigma_k^2)} $\n",
    "\n",
    "    The joint probability is then:\n",
    "\n",
    "    $ P_{ij} = \\frac{p_{ij} + p_{ji}}{2N} $\n",
    "\n",
    "2. **Pairwise Similarities in Low-Dimensional Space:**\n",
    "\n",
    "    Compute the probability $ q_{ij} $ in the lower-dimensional space using a t-distribution:\n",
    "\n",
    "    $ q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}} $\n",
    "\n",
    "3. **Minimize Divergence Between Distributions:**\n",
    "\n",
    "    Use Kullback-Leibler (KL) Divergence as the objective function to match the high-dimensional probabilities ($ P $) with the low-dimensional ones ($ Q $):\n",
    "\n",
    "    $ KL(P \\| Q) = \\sum_{i \\neq j} P_{ij} \\log \\frac{P_{ij}}{Q_{ij}} $\n",
    "\n",
    "    Optimize using gradient descent to adjust the low-dimensional embeddings.\n",
    "\n",
    "### Key Properties of t-SNE:\n",
    "\n",
    "- Focuses on preserving local neighborhoods.\n",
    "- Uses t-distribution in low-dimensional space, which helps spread points further apart to avoid crowding.\n",
    "\n",
    "### Applications of t-SNE:\n",
    "\n",
    "- Visualizing high-dimensional datasets like word embeddings, image data, or gene expressions.\n",
    "- Exploring data clusters or patterns before applying other algorithms.\n",
    "\n",
    "### Advantages of t-SNE:\n",
    "\n",
    "- Handles non-linear relationships well.\n",
    "- Effective at revealing local patterns in data.\n",
    "- Great for visualization of complex datasets.\n",
    "\n",
    "### Disadvantages of t-SNE:\n",
    "\n",
    "- Computationally expensive, especially for large datasets.\n",
    "- Results can vary between runs (non-deterministic).\n",
    "- Poor at preserving global structures (focuses on local relationships).\n",
    "\n",
    "# PCA vs. t-SNE\n",
    "\n",
    "| Aspect         | PCA                              | t-SNE                               |\n",
    "|----------------|----------------------------------|-------------------------------------|\n",
    "| Type           | Linear dimensionality reduction. | Non-linear dimensionality reduction.|\n",
    "| Goal           | Preserve variance in data.       | Preserve local neighborhood structure. |\n",
    "| Output         | New axes (principal components). | Visualization of clusters.          |\n",
    "| Scalability    | Fast and scalable to large datasets. | Computationally expensive.          |\n",
    "| Global Structure | Preserves global structure.     | Focuses on local structure.         |\n",
    "| Deterministic  | Yes (same result every time).    | No (varies between runs).           |\n",
    "| Applications   | Feature extraction, compression, denoising. | Data visualization, cluster exploration. |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "- Use PCA for reducing dimensions while retaining variance.\n",
    "- Use t-SNE for visualizing clusters and non-linear patterns in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of PCA\n",
    "\n",
    "**Scenario: Image Compression**\n",
    "\n",
    "Imagine we have grayscale images of handwritten digits (like in the MNIST dataset). Each image is 28x28 pixels, meaning there are 784 features (one for each pixel).\n",
    "\n",
    "The goal is to reduce the dimensions while retaining the most critical information.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Original Data:**\n",
    "\n",
    "\tEach image is represented as a vector of 784 features.\n",
    "\n",
    "2. **Apply PCA:**\n",
    "\n",
    "\t- Compute the covariance matrix.\n",
    "\t- Find the eigenvectors and eigenvalues.\n",
    "\t- Select the top components (e.g., 50).\n",
    "\t- Project the data onto these 50 components.\n",
    "\n",
    "3. **Result:**\n",
    "\n",
    "\t- Each image is now represented by only 50 features (instead of 784), while retaining ~90% of the variance.\n",
    "\t- The reduced data can be stored and transmitted more efficiently.\n",
    "\n",
    "4. **Reconstruction:**\n",
    "\n",
    "\tWhen we want to reconstruct the image from the reduced data, it will look similar to the original but with some loss of detail.\n",
    "\n",
    "**Visualization of PCA on 2D Data:**\n",
    "\n",
    "Imagine a dataset of points in 3D space (features: height, weight, and age). PCA reduces it to 2D:\n",
    "\n",
    "- Original data: (height, weight, age).\n",
    "- After PCA: (PC1, PC2), where PC1 and PC2 capture most of the variance.\n",
    "\n",
    "### Example of t-SNE\n",
    "\n",
    "**Scenario: Visualizing Word Embeddings**\n",
    "\n",
    "Suppose you have word embeddings generated by models like Word2Vec or GloVe. Each word is represented as a vector in 300-dimensional space.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **High-Dimensional Space:**\n",
    "\n",
    "\tWords like “king,” “queen,” and “castle” are points in a 300-dimensional space, where similar words are closer together.\n",
    "\n",
    "2. **Apply t-SNE:**\n",
    "\n",
    "\t- Compute pairwise similarities in 300D space using Gaussian probabilities.\n",
    "\t- Map the high-dimensional points to a 2D or 3D space while preserving local relationships.\n",
    "\n",
    "3. **Result:**\n",
    "\n",
    "\t- Words with similar meanings (e.g., “king” and “queen”) will form clusters in the 2D plot.\n",
    "\t- You might see distinct clusters for animals, countries, professions, etc.\n",
    "\n",
    "**Visualization of t-SNE on MNIST Dataset:**\n",
    "\n",
    "- Input: High-dimensional pixel data of handwritten digits (e.g., 784D).\n",
    "- Output: A 2D plot where points representing the same digit (e.g., “0” or “1”) form distinct clusters.\n",
    "  - Cluster 1: All “0”s.\n",
    "  - Cluster 2: All “1”s.\n",
    "\n",
    "### PCA vs. t-SNE Examples in Real-World\n",
    "\n",
    "| Use Case              | PCA Example                                                   | t-SNE Example                                               |\n",
    "|-----------------------|---------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| Gene Expression Data  | Reduce thousands of gene features to top components for further analysis. | Visualize clusters of similar gene expressions in 2D.       |\n",
    "| Image Data            | Compress images by reducing pixel dimensions (e.g., 784D to 50D). | Explore clusters of images (e.g., faces, objects).          |\n",
    "| Text Data             | Reduce TF-IDF matrix for topic modeling.                      | Visualize clusters of documents or word embeddings.         |\n",
    "| Customer Segmentation | Reduce purchase behavior features to key components.          | Visualize customer groups (e.g., high vs. low spenders).    |\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **PCA:** Focuses on linear transformations and variance preservation, ideal for preprocessing and compression.\n",
    "2. **t-SNE:** Non-linear and primarily for visualization to explore local patterns and clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions on PCA and t-SNE\n",
    "\n",
    "## PCA Questions\n",
    "\n",
    "1. **What is PCA?**\n",
    "    - PCA is a dimensionality reduction technique that transforms data into a new coordinate system defined by principal components, preserving as much variance as possible.\n",
    "\n",
    "2. **How does PCA work?**\n",
    "    - PCA standardizes data, computes the covariance matrix, finds eigenvectors and eigenvalues, ranks them, and projects data onto the top principal components.\n",
    "\n",
    "3. **What are principal components?**\n",
    "    - Principal components are orthogonal vectors that represent directions of maximum variance in the data.\n",
    "\n",
    "4. **What is the role of eigenvalues in PCA?**\n",
    "    - Eigenvalues indicate the amount of variance explained by each principal component. Larger eigenvalues mean more variance is captured.\n",
    "\n",
    "5. **What are the limitations of PCA?**\n",
    "    - PCA assumes linearity, is sensitive to scaling, and may lose interpretability of original features.\n",
    "\n",
    "6. **When would you use PCA?**\n",
    "    - To reduce dimensionality, speed up computations, or visualize high-dimensional data.\n",
    "\n",
    "7. **How do you decide the number of principal components to keep?**\n",
    "    - Use the explained variance ratio and select enough components to retain ~90-95% of the variance.\n",
    "\n",
    "8. **What are some practical applications of PCA?**\n",
    "    - Image compression, feature extraction, noise reduction, and exploratory data analysis.\n",
    "\n",
    "9. **How does PCA differ from linear regression?**\n",
    "    - PCA identifies new axes that maximize variance, while linear regression predicts a target variable based on input features.\n",
    "\n",
    "10. **What type of data is suitable for PCA?**\n",
    "     - Numeric, continuous data; PCA does not work well with categorical features unless encoded numerically.\n",
    "\n",
    "## t-SNE Questions\n",
    "\n",
    "1. **What is t-SNE?**\n",
    "    - t-SNE is a non-linear dimensionality reduction technique used for visualizing high-dimensional data in 2D or 3D.\n",
    "\n",
    "2. **How does t-SNE differ from PCA?**\n",
    "    - PCA is linear and preserves global variance, while t-SNE is non-linear and focuses on preserving local neighborhood structure.\n",
    "\n",
    "3. **How does t-SNE work?**\n",
    "    - t-SNE computes pairwise similarities in high-dimensional space and maps them to low-dimensional space using probabilities and minimizes KL divergence.\n",
    "\n",
    "4. **What is the role of the t-distribution in t-SNE?**\n",
    "    - The t-distribution prevents the crowding problem in low-dimensional space by spreading points apart.\n",
    "\n",
    "5. **What are the main hyperparameters of t-SNE?**\n",
    "    - Perplexity (controls local vs. global focus), learning rate, and number of iterations.\n",
    "\n",
    "6. **What are the limitations of t-SNE?**\n",
    "    - Computationally expensive, non-deterministic, and poor at preserving global structure.\n",
    "\n",
    "7. **When would you use t-SNE?**\n",
    "    - For visualizing high-dimensional data clusters, such as word embeddings or gene expression data.\n",
    "\n",
    "8. **Why is t-SNE non-deterministic?**\n",
    "    - It initializes embeddings randomly and uses gradient descent, leading to slightly different results on each run.\n",
    "\n",
    "9. **How does perplexity affect t-SNE?**\n",
    "    - Perplexity balances the trade-off between preserving local vs. global structure; common values are 5–50.\n",
    "\n",
    "10. **Can t-SNE handle large datasets?**\n",
    "     - Not efficiently. For large datasets, techniques like PCA+t-SNE (dimensionality reduction with PCA first) or UMAP are better options.\n",
    "\n",
    "## Advanced PCA and t-SNE Questions\n",
    "\n",
    "1. **How do PCA and t-SNE complement each other?**\n",
    "    - PCA is often used to reduce dimensions before applying t-SNE, speeding up computations and reducing noise.\n",
    "\n",
    "2. **What are eigenvalues and eigenvectors, and how are they used in PCA?**\n",
    "    - Eigenvalues represent variance captured by principal components; eigenvectors define the direction of those components.\n",
    "\n",
    "3. **What are the alternatives to PCA and t-SNE?**\n",
    "    - PCA: Linear Discriminant Analysis (LDA), Factor Analysis.\n",
    "    - t-SNE: UMAP, Isomap.\n",
    "\n",
    "4. **Why does t-SNE minimize KL divergence?**\n",
    "    - To match pairwise probabilities between high- and low-dimensional spaces, preserving local structure.\n",
    "\n",
    "5. **How does t-SNE handle high-dimensional noise?**\n",
    "    - It is sensitive to noise. Preprocessing steps like PCA or filtering can improve t-SNE performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other dimensionality reduction techniques. \n",
    "\n",
    "Depending on the type of data and the relationships you want to preserve. Below is a list of popular methods, categorized into linear and non-linear approaches:\n",
    "\n",
    "### Linear Dimensionality Reduction Methods\n",
    "\n",
    "1. **Linear Discriminant Analysis (LDA):**\n",
    "\t- **How it works:** Finds a linear combination of features that separates classes in a dataset. Maximizes the distance between class means and minimizes the spread within each class.\n",
    "\t- **Use case:** Supervised learning problems for classification tasks.\n",
    "\t- **Limitation:** Assumes data is linearly separable and works only with labeled data.\n",
    "\n",
    "2. **Factor Analysis:**\n",
    "\t- **How it works:** Assumes that observed data is generated by a set of unobserved latent variables and some noise. Reduces dimensions by modeling the covariance structure of the data.\n",
    "\t- **Use case:** Psychometrics, social sciences, and marketing data.\n",
    "\t- **Limitation:** Works best when the data fits the factor analysis model assumptions.\n",
    "\n",
    "3. **Independent Component Analysis (ICA):**\n",
    "\t- **How it works:** Separates mixed signals into statistically independent components. Often used in signal processing or for identifying hidden factors.\n",
    "\t- **Use case:** Blind source separation, such as separating audio signals (e.g., cocktail party problem).\n",
    "\t- **Limitation:** Sensitive to noise and requires careful preprocessing.\n",
    "\n",
    "4. **Multi-Dimensional Scaling (MDS):**\n",
    "\t- **How it works:** Preserves pairwise distances between points in the high-dimensional space when mapping to a lower-dimensional space.\n",
    "\t- **Use case:** Visualizing data with meaningful distance metrics.\n",
    "\t- **Limitation:** Computationally expensive for large datasets.\n",
    "\n",
    "### Non-Linear Dimensionality Reduction Methods\n",
    "\n",
    "1. **t-SNE (t-Distributed Stochastic Neighbor Embedding):**\n",
    "\t- **How it works:** Focuses on preserving local structures by minimizing KL divergence between high- and low-dimensional distributions.\n",
    "\t- **Use case:** Visualization of clusters in high-dimensional datasets.\n",
    "\t- **Limitation:** Non-deterministic and computationally expensive.\n",
    "\n",
    "2. **UMAP (Uniform Manifold Approximation and Projection):**\n",
    "\t- **How it works:** Similar to t-SNE but faster and better at preserving both local and global structures.\n",
    "\t- **Use case:** Large-scale high-dimensional data visualization.\n",
    "\t- **Limitation:** Hyperparameters require careful tuning.\n",
    "\n",
    "3. **Isomap (Isometric Mapping):**\n",
    "\t- **How it works:** Computes geodesic distances between points on a manifold and preserves them in the low-dimensional space.\n",
    "\t- **Use case:** Non-linear relationships in manifold-like data.\n",
    "\t- **Limitation:** Sensitive to noise and outliers.\n",
    "\n",
    "4. **Autoencoders (Deep Learning-based):**\n",
    "\t- **How it works:** Neural networks learn to encode data into a compressed representation and then decode it back to the original input.\n",
    "\t- **Use case:** Complex non-linear dimensionality reduction for large datasets.\n",
    "\t- **Limitation:** Requires large datasets and significant training time.\n",
    "\n",
    "5. **Locally Linear Embedding (LLE):**\n",
    "\t- **How it works:** Preserves local relationships by reconstructing each point using its neighbors.\n",
    "\t- **Use case:** Manifold learning in non-linear data.\n",
    "\t- **Limitation:** Computationally expensive and sensitive to noise.\n",
    "\n",
    "6. **Kernel PCA:**\n",
    "\t- **How it works:** Extends PCA to non-linear relationships by applying the kernel trick to project data into higher dimensions before performing PCA.\n",
    "\t- **Use case:** Non-linear data where standard PCA fails.\n",
    "\t- **Limitation:** Requires kernel selection and parameter tuning.\n",
    "\n",
    "7. **Laplacian Eigenmaps:**\n",
    "\t- **How it works:** Uses graph-based methods to preserve local relationships between points by constructing a similarity graph.\n",
    "\t- **Use case:** Non-linear data with underlying graph-like structures.\n",
    "\t- **Limitation:** Sensitive to graph construction and requires careful tuning.\n",
    "\n",
    "### Comparison of Methods\n",
    "\n",
    "| Method         | Linear/Non-Linear | Preserves                | Use Case                          |\n",
    "|----------------|-------------------|--------------------------|-----------------------------------|\n",
    "| PCA            | Linear            | Global variance          | Feature reduction, compression.   |\n",
    "| LDA            | Linear            | Class separability       | Classification tasks.             |\n",
    "| t-SNE          | Non-linear        | Local neighborhood       | Visualization of clusters.        |\n",
    "| UMAP           | Non-linear        | Local & global structure | Large-scale data visualization.   |\n",
    "| Isomap         | Non-linear        | Geodesic distances       | Manifold learning.                |\n",
    "| Kernel PCA     | Non-linear        | Variance (in kernel space) | Non-linear feature extraction.    |\n",
    "| Autoencoders   | Non-linear        | Learned representation   | Complex feature extraction.       |\n",
    "\n",
    "### Key Takeaway:\n",
    "\n",
    "- Use PCA, LDA, or ICA for linear problems.\n",
    "- Use t-SNE, UMAP, or Isomap for non-linear relationships or visualization.\n",
    "- Use Autoencoders for deep learning applications or very complex data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
