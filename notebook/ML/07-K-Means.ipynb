{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering\n",
    "\n",
    "It is one of the simplest and most commonly used unsupervised learning algorithms that solves the well-known clustering problem. Here's an in-depth explanation along with an example:\n",
    "\n",
    "## Core Concepts:\n",
    "\n",
    "1. **Objective**:\n",
    "    - The goal is to partition n observations into k clusters where each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This mean is called the cluster centroid.\n",
    "\n",
    "2. **Algorithm Steps**:\n",
    "    - **Initialization**: Choose k initial centroids randomly or using methods like K-means++ for better initial placement.\n",
    "    - **Assignment Step**: Assign each data point to the nearest centroid, forming k clusters.\n",
    "    - **Update Step**: Recalculate the centroids of each cluster as the mean of all points in that cluster.\n",
    "    - **Iteration**: Repeat the assignment and update steps until the centroids no longer move significantly (convergence) or a maximum number of iterations is reached.\n",
    "\n",
    "3. **Distance Metric**:\n",
    "    - Typically Euclidean distance is used, but other distance metrics like Manhattan could be applied depending on the nature of the data.\n",
    "\n",
    "## Algorithm in Detail:\n",
    "\n",
    "1. **Choose Initial Centroids**:\n",
    "    - Random selection or use K-means++ for better results. K-means++ selects initial centroids to be distant from each other, reducing the chance of poor clustering due to bad initial guesses.\n",
    "\n",
    "2. **Assign Points to Clusters**:\n",
    "    - For each data point, find the nearest centroid using the chosen distance metric. Assign the point to that cluster.\n",
    "\n",
    "3. **Update Centroids**:\n",
    "    - Compute the new centroid for each cluster by averaging all points in that cluster. This is the mean position of the points in the cluster.\n",
    "\n",
    "4. **Repeat**:\n",
    "    - Go back to step 2 until the centroids don't move significantly, or a predefined number of iterations is reached. \n",
    "\n",
    "## Example:\n",
    "\n",
    "Let's use a simple 2D dataset to illustrate:\n",
    "\n",
    "**Dataset**:\n",
    "\n",
    "| Point | X | Y |\n",
    "|-------|---|---|\n",
    "| A     | 1 | 2 |\n",
    "| B     | 2 | 3 |\n",
    "| C     | 3 | 3 |\n",
    "| D     | 5 | 5 |\n",
    "| E     | 6 | 6 |\n",
    "| F     | 7 | 7 |\n",
    "\n",
    "**Steps with k=2**:\n",
    "\n",
    "1. **Initialization**: \n",
    "    - Let's say we randomly choose points A(1,2) and D(5,5) as initial centroids.\n",
    "\n",
    "2. **Assignment Step**:\n",
    "    - A to A: 0 (itself) -> Cluster 1\n",
    "    - B to A: $ \\sqrt{(2-1)^2 + (3-2)^2} \\approx 1.41 $, to D: $ \\sqrt{(2-5)^2 + (3-5)^2} \\approx 3.61 $ -> Cluster 1\n",
    "    - C to A: $ \\sqrt{(3-1)^2 + (3-2)^2} \\approx 2.24 $, to D: $ \\sqrt{(3-5)^2 + (3-5)^2} \\approx 2.83 $ -> Cluster 1\n",
    "    - D to A: 3.61, to D: 0 -> Cluster 2\n",
    "    - E to A: $ \\sqrt{(6-1)^2 + (6-2)^2} \\approx 5.83 $, to D: $ \\sqrt{(6-5)^2 + (6-5)^2} \\approx 1.41 $ -> Cluster 2\n",
    "    - F to A: $ \\sqrt{(7-1)^2 + (7-2)^2} \\approx 7.81 $, to D: $ \\sqrt{(7-5)^2 + (7-5)^2} \\approx 2.83 $ -> Cluster 2\n",
    "\n",
    "    So, Clusters are now:\n",
    "    - **Cluster 1**: {A, B, C}\n",
    "    - **Cluster 2**: {D, E, F}\n",
    "\n",
    "3. **Update Centroids**:\n",
    "    - New centroid for Cluster 1: Mean of (1,2), (2,3), (3,3) => (2, 2.67)\n",
    "    - New centroid for Cluster 2: Mean of (5,5), (6,6), (7,7) => (6, 6)\n",
    "\n",
    "4. **Repeat Assignment with New Centroids**:\n",
    "    - Re-calculate distances with new centroids:\n",
    "    - A: Cluster 1 (distance ≈ 0.78) vs. Cluster 2 (distance ≈ 5.39) -> Cluster 1\n",
    "    - B: Cluster 1 (distance ≈ 0.58) vs. Cluster 2 (distance ≈ 4.47) -> Cluster 1\n",
    "    - C: Cluster 1 (distance ≈ 0.58) vs. Cluster 2 (distance ≈ 3.61) -> Cluster 1\n",
    "    - D: Cluster 1 (distance ≈ 3.04) vs. Cluster 2 (distance ≈ 1) -> Cluster 2\n",
    "    - E: Cluster 1 (distance ≈ 4.24) vs. Cluster 2 (distance ≈ 1) -> Cluster 2\n",
    "    - F: Cluster 1 (distance ≈ 5.39) vs. Cluster 2 (distance ≈ 1.41) -> Cluster 2\n",
    "\n",
    "    Clusters are now:\n",
    "    - **Cluster 1**: {A, B, C}\n",
    "    - **Cluster 2**: {D, E, F}\n",
    "\n",
    "    The centroids would be recalculated again, but in this example, they remain the same, indicating convergence.\n",
    "\n",
    "#**Practical Considerations:\n",
    "\n",
    "- **Choosing k**: Often done via methods like the Elbow method (looking for the \"elbow\" point in the sum of squared errors vs. k plot) or silhouette analysis.\n",
    "- **Initialization Sensitivity**: K-means is sensitive to initial centroid choice; K-means++ can mitigate this.\n",
    "- **Handling Noise and Outliers**: Standard K-means does not handle these well; variants like K-medoids are more robust.\n",
    "- **Scalability**: For large datasets, mini-batch K-means can be used for efficiency.\n",
    "\n",
    "K-means is widely used due to its simplicity and efficiency but remember its limitations, particularly its assumption of spherical clusters and sensitivity to initial conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions\n",
    "\n",
    "**1. What is K-Means Clustering?**\n",
    "\n",
    "K-Means is an unsupervised machine learning algorithm that partitions data into clusters by minimizing the sum of squared distances between data points and their cluster centroids.\n",
    "\n",
    "**2. How does the K-Means algorithm work?**\n",
    "\n",
    "1. Initialize random centroids.\n",
    "2. Assign each data point to the nearest centroid.\n",
    "3. Update centroids as the mean of assigned points.\n",
    "4. Repeat steps 2 and 3 until centroids stabilize (no significant change).\n",
    "\n",
    "**3. What is the objective function of K-Means?**\n",
    "\n",
    "Minimize the intra-cluster sum of squares (WCSS):\n",
    "\n",
    "$\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$\n",
    "\n",
    "where $\\mu_i$ is the centroid of cluster $C_i$.\n",
    "\n",
    "**4. How do you choose the value of $k$ in K-Means?**\n",
    "\n",
    "- Use the Elbow Method: Plot WCSS vs. $k$. The optimal $k$ is at the “elbow” point where WCSS reduction slows down.\n",
    "- Use Silhouette Score to evaluate cluster quality.\n",
    "\n",
    "**5. What are the limitations of K-Means?**\n",
    "\n",
    "- Requires predefining $k$.\n",
    "- Sensitive to initial centroid selection.\n",
    "- Assumes spherical clusters (fails for complex shapes).\n",
    "- Sensitive to outliers.\n",
    "\n",
    "**6. What are the assumptions of K-Means?**\n",
    "\n",
    "- Clusters are spherical and equally sized.\n",
    "- Data points are closer to their cluster centroid than to other centroids.\n",
    "- Clusters are well-separated.\n",
    "\n",
    "**7. What are some common methods to initialize centroids?**\n",
    "\n",
    "- Random Initialization: Randomly select $k$ data points as centroids.\n",
    "- K-Means++: Improves initialization by choosing centroids that are far apart, reducing the chance of poor clustering.\n",
    "\n",
    "**8. What is K-Means++ initialization, and why is it used?**\n",
    "\n",
    "- K-Means++ ensures better centroid initialization by selecting initial centroids probabilistically based on distance, reducing convergence time and improving accuracy.\n",
    "\n",
    "**9. What is the difference between hard and soft clustering?**\n",
    "\n",
    "- Hard Clustering: Each data point belongs to exactly one cluster (e.g., K-Means).\n",
    "- Soft Clustering: Each data point can belong to multiple clusters with probabilities (e.g., Gaussian Mixture Models).\n",
    "\n",
    "**10. How do you handle outliers in K-Means?**\n",
    "\n",
    "- Use robust clustering methods like K-Medoids or DBSCAN.\n",
    "- Preprocess data by removing or scaling outliers.\n",
    "\n",
    "**11. What are the advantages of K-Means?**\n",
    "\n",
    "- Simple and easy to implement.\n",
    "- Scales well for large datasets.\n",
    "- Computationally efficient with $O(nkt)$, where $t$ is the number of iterations.\n",
    "\n",
    "**12. What are the disadvantages of K-Means?**\n",
    "\n",
    "- Sensitive to initial centroids and outliers.\n",
    "- Requires specifying $k$.\n",
    "- Struggles with overlapping or non-spherical clusters.\n",
    "\n",
    "**13. What are the stopping criteria for K-Means?**\n",
    "\n",
    "- Centroids stop changing significantly.\n",
    "- Maximum number of iterations reached.\n",
    "- Decrease in WCSS is below a threshold.\n",
    "\n",
    "**14. What is the difference between K-Means and K-Medoids?**\n",
    "\n",
    "- K-Means: Uses the mean as the centroid (sensitive to outliers).\n",
    "- K-Medoids: Uses the median as the centroid (more robust to outliers).\n",
    "\n",
    "**15. How does scaling data affect K-Means?**\n",
    "\n",
    "- K-Means is sensitive to feature scaling because it uses Euclidean distance. Standardize or normalize data to ensure fair clustering.\n",
    "\n",
    "**16. What is cluster inertia in K-Means?**\n",
    "\n",
    "- Inertia is the total WCSS, measuring how compact clusters are. Lower inertia indicates better clustering.\n",
    "\n",
    "**17. Can K-Means handle categorical data?**\n",
    "\n",
    "- K-Means is designed for numerical data. For categorical data, use algorithms like K-Modes or K-Prototypes.\n",
    "\n",
    "**18. What is the Silhouette Score in K-Means?**\n",
    "\n",
    "- The Silhouette Score measures cluster quality, ranging from -1 to 1:\n",
    "\n",
    "- $1$: Points are well-matched to their cluster.\n",
    "- $0$: Points are on cluster boundaries.\n",
    "- $-1$: Points are in the wrong cluster.\n",
    "\n",
    "**19. What are the alternatives to K-Means for non-spherical clusters?**\n",
    "\n",
    "- DBSCAN: Density-based clustering for arbitrary shapes.\n",
    "- Hierarchical Clustering: Builds a dendrogram for hierarchical clusters.\n",
    "- Gaussian Mixture Models: Probabilistic clustering.\n",
    "\n",
    "**20. How do you evaluate K-Means clustering?**\n",
    "\n",
    "- Internal Metrics: Silhouette Score, Inertia (WCSS).\n",
    "- External Metrics: Purity, Adjusted Rand Index (ARI), or Normalized Mutual Information (NMI) when true labels are known.\n",
    "\n",
    "These questions test a mix of theoretical understanding and practical knowledge of K-Means clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced interview questions\n",
    "\n",
    "1. **What are the challenges of using K-Means on high-dimensional data?**\n",
    "\t- Curse of Dimensionality: Distances between points become less meaningful.\n",
    "\t- Clusters may overlap due to sparsity in high dimensions.\n",
    "\t- Mitigation: Use PCA or t-SNE to reduce dimensionality before applying K-Means.\n",
    "\n",
    "2. **How does K-Means handle imbalanced cluster sizes?**\n",
    "\t- K-Means may perform poorly on imbalanced clusters as it tries to minimize the sum of squared distances, leading to smaller clusters being merged into larger ones.\n",
    "\t- Solution: Use algorithms like DBSCAN or modify the objective function.\n",
    "\n",
    "3. **What happens if two centroids overlap during K-Means clustering?**\n",
    "\t- If centroids overlap, some clusters may become empty. In such cases, K-Means randomly reinitializes the empty cluster’s centroid. This may cause instability in convergence.\n",
    "\n",
    "4. **Explain the time complexity of K-Means. How can it be optimized?**\n",
    "\t- Time complexity: $O(n \\cdot k \\cdot i \\cdot d)$, where:\n",
    "\t\t- $n$: Number of points.\n",
    "\t\t- $k$: Number of clusters.\n",
    "\t\t- $i$: Number of iterations.\n",
    "\t\t- $d$: Dimensionality of data.\n",
    "\t- Optimization: Use Mini-Batch K-Means or approximate nearest neighbors.\n",
    "\n",
    "5. **How can you detect if K-Means is stuck in a local minimum?**\n",
    "\t- Monitor WCSS across runs with different initializations.\n",
    "\t- Use K-Means++ to reduce the chance of poor initialization.\n",
    "\t- Compare clustering results with domain knowledge or external validation.\n",
    "\n",
    "6. **How do you handle categorical or mixed data in K-Means?**\n",
    "\t- K-Means doesn’t work well with categorical data due to its reliance on Euclidean distance. Use:\n",
    "\t\t- K-Modes: For purely categorical data, using mode instead of mean.\n",
    "\t\t- K-Prototypes: For mixed data types, combining numeric and categorical features.\n",
    "\n",
    "7. **What is Bisecting K-Means? How does it improve clustering?**\n",
    "\t- Bisecting K-Means splits data recursively:\n",
    "\t\t1. Choose a cluster and split it into two using K-Means with $k=2$.\n",
    "\t\t2. Select the split that minimizes WCSS.\n",
    "\t\t3. Repeat until $k$ clusters are formed.\n",
    "\t- It often produces better results for hierarchical clustering.\n",
    "\n",
    "8. **Why is Euclidean distance used in K-Means? Can it be replaced?**\n",
    "\t- K-Means uses Euclidean distance to measure similarity, which works well for spherical clusters.\n",
    "\t- Alternatives: Manhattan distance, Cosine similarity, or Mahalanobis distance for different data distributions.\n",
    "\n",
    "9. **How do you deal with outliers in K-Means?**\n",
    "\t- Outliers can distort centroids and clustering performance. Approaches include:\n",
    "\t\t- Preprocessing: Remove or cap outliers using statistical techniques.\n",
    "\t\t- Using K-Medoids: Less sensitive to outliers as it uses medians instead of means.\n",
    "\n",
    "10. **How does Mini-Batch K-Means differ from standard K-Means?**\n",
    "\t- Mini-Batch K-Means processes a random subset (mini-batch) of data in each iteration, instead of the entire dataset.\n",
    "\t\t- Pros: Faster and scalable for large datasets.\n",
    "\t\t- Cons: May have slightly lower accuracy than full K-Means.\n",
    "\n",
    "11. **Can K-Means clustering be applied to streaming data? How?**\n",
    "\t- Yes, by using Online K-Means, which updates centroids incrementally as new data arrives. Mini-Batch K-Means is also effective for large-scale or streaming data.\n",
    "\n",
    "12. **What are the implications of non-convex clusters for K-Means?**\n",
    "\t- K-Means assumes spherical clusters. For non-convex clusters (e.g., crescent shapes), K-Means fails to separate them accurately.\n",
    "\t- Solution: Use density-based methods like DBSCAN or kernelized clustering techniques.\n",
    "\n",
    "13. **What is the relationship between K-Means and Expectation-Maximization (EM)?**\n",
    "\t- K-Means can be viewed as a special case of EM for Gaussian Mixture Models (GMM) with:\n",
    "\t\t- Equal variances for all components.\n",
    "\t\t- Hard cluster assignments (instead of soft probabilities in GMM).\n",
    "\n",
    "14. **How would you evaluate K-Means clustering if ground truth labels are unavailable?**\n",
    "\t- Use internal metrics, such as:\n",
    "\t\t- Silhouette Score: Measures separation and compactness of clusters.\n",
    "\t\t- Davies-Bouldin Index: Lower values indicate better clustering.\n",
    "\t\t- Calinski-Harabasz Index: Higher values indicate better-defined clusters.\n",
    "\n",
    "15. **Why does K-Means struggle with overlapping clusters?**\n",
    "\t- K-Means assigns points to the nearest centroid based on Euclidean distance, which fails to account for overlaps in data distributions. Gaussian Mixture Models (GMM) handle overlaps better using soft clustering.\n",
    "\n",
    "16. **What are centroid initialization techniques besides K-Means++?**\n",
    "\t- Forgy Method: Randomly selects $k$ points from the dataset as centroids.\n",
    "\t- Random Partition: Assigns data points to clusters randomly, then calculates centroids.\n",
    "\t- Density-Based Initialization: Uses density information to initialize centroids.\n",
    "\n",
    "17. **How does cluster size affect the results of K-Means?**\n",
    "\t- K-Means tends to favor clusters of similar sizes due to the nature of its distance-based optimization. Unequal cluster sizes can lead to poor performance or merging of smaller clusters.\n",
    "\n",
    "18. **What are the limitations of the Elbow Method?**\n",
    "\t- The elbow point may not always be distinct, making $k$ ambiguous.\n",
    "\t- Assumes clusters are well-separated and spherical.\n",
    "\t- Alternatives: Silhouette Score or Gap Statistic.\n",
    "\n",
    "19. **How do you use K-Means for image compression?**\n",
    "\t1. Treat each pixel as a data point in $3D$ (RGB space).\n",
    "\t2. Apply K-Means to cluster pixels into $k$ colors.\n",
    "\t3. Replace pixel colors with their corresponding cluster centroids.\n",
    "\t- This reduces the number of unique colors in the image.\n",
    "\n",
    "20. **How does dimensionality reduction affect K-Means clustering?**\n",
    "\t- Dimensionality reduction (e.g., PCA) removes noise and redundant features, improving clustering performance. However, information loss may occur if too many dimensions are reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
